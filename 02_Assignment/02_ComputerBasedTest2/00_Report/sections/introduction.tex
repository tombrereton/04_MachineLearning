\section{Overall Task}
We are asked to determine if a patient is diseased or healthy from a given data set. A new test is in development and measures the concentration of 2 chemicals in urine samples. 500 patients provide urine samples and also undergo a blood tests to classify if they are diseased or healthy. Thus, we have a 500 by 2 data set which is plotted in Figure \ref{fig:trainingData}. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{images/MLtrainingData}
	\caption{Training Data for the 4 methods outlined in this report. Note the imbalanced amount of healthy to diseased patients.}
	\label{fig:trainingData}
\end{figure}

We use this data set to train the 4 following methods. 

\begin{itemize}
	\item Maximum Likelihood (ML) - Appendix A
	\item Maximum Likelihood without Naive assumption (MLWON) - Appendix B
	\item Maximum A Posteriori (MAP) - Appendix C
	\item Maximum A Posteriori without Naive assumption(MAPWON) - Appendix D
\end{itemize}

The analysis was done in Matlab and the code listings for each method are found in the Appendices as described above. Appendix E contains a code listing for a program comparing the classification of each method.

Once trained, we are tasked with classifying 2000 new urine samples. We use the 4 methods outlined previously to determine whether a `patient' is diseased or healthy. 

It is helpful to state a hypothesis for assessment. Considering the data in Figure \ref{fig:trainingData} the hypothesis is, an increase in concentration of chemical 1 and 2 increases the likelihood of a patient being diagnosed as diseased.



\section{Comparison of Naive vs Without Naive}{\label{s1}
	
\subsection{Introduction}\label{Int}
For simplicity we consider only the Maximum Likelihood (ML) when comparing the Naive vs without Naive. The difference between ML and Maximum A Posteriori will be discussed later in the report.

In Task 1, we are asked to ``train the Bayesian classifier, with maximum likelihood estimate (ML), on the training data with and without the Naive assumption.'' The data set is the one shown in \ref{fig:trainingData}. Once trained we classify 2000 new data points (patients).

To train ML with the Naive assumption we first compute the mean and variance of the data for each class and attribute. As it is Naive, we assume the attributes (chemical 1 and 2) are independent. Armed with the mean, variance, and the new data set we use the Gaussian expression to compute the likelihood of each data point as either diseased or healthy. In this case, we are looking at the maximum likelihood and thus, only use class-conditional likelihood to classify. Classifying without the Naive assumption is show in Figure \ref{fig:ML}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{images/MLnewData}
	\caption{Maximum Likelihood classification of new data}
	\label{fig:ML}
\end{figure}

To train ML without the Naive assumption is similar to before. We compute the mean as normal but this method requires the covariance not variance of the classes and attributes. This assumes a dependence between the attributes which is useful to look at if there are not many attributes. We then compute the maximum likelihood as normal but accounting for the 3 dimensional matrix due to the covariance of each class. Classifying with the Naive assumption is shown in Figure \ref{fig:MLWON}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{images/MLWONnewData}
	\caption{Maximum Likelihood without Naive assumption classification of new data}
	\label{fig:MLWON}
\end{figure}

\subsection{The means}
The mean is the average of an attribute. If we consider a normal distribution for 1 attribute, it is the value at which the distribution is centred.

Consider Figure \ref{fig:trainingData} and Table \ref{t:mean}. We can see the means of each class are far apart, with the diseased mean at point (5.90, 13.13) and the healthy mean at (3.02, 8.06). Relative to the scale of this data, the means are indeed far apart. The mean is represented by the centre of each cluster in Figure \ref{fig:trainingData}.

\begin{table}[h]
	\centering
	\caption{Mean of Chemical Concentrations for ML}
	\label{t:mean}
	\begin{tabular}{lrr}
		\hline
		\textbf{Class} & \textbf{Chemical 1} & \textbf{Chemical 2} \\ \hline
		Diseased & 5.90 & 13.113 \\
		Healthy & 3.02 & 8.06  \\
	\end{tabular}
\end{table}

It should be noted the means remain the same for with and without the Naive assumption, also for ML vs MAP. This is because we calculate the mean on the training data, thus, new data or the method of analysis has no affect.

\subsection{The Covariance}
The covariance defines the shape and spread of the data. If we consider the normal distribution for 1 attribute again, a large covariance (or variance in this case), means it will have a large spread from the centre. 

Looking at Figures \ref{fig:cml1} and \ref{fig:cml2}, the contour plots of each class-conditional describe the covariance by their shape. For the naive assumption, the healthy and diseased contour plots are more or less circular. This is because we assume independence between the 2 attributes. Tables \ref{t:var1} and \ref{t:var2} show the covariances for Diseased and Healthy patients respectively. Notice the off diagonal elements are 0, meaning there is no relationship between attribute 1 and 2 i.e. a circular density contour. 

\begin{figure}[h!] 
	\centering
	\begin{subfigure}[b]{.40\textwidth}
		\includegraphics[width=\textwidth]{MLclassCondContoursDiseased.png}
		\caption{ML class conditional 1 density contours}
		\label{fig:cml1}
	\end{subfigure}
	\begin{subfigure}[b]{.40\textwidth}
		\includegraphics[width=\textwidth]{MLclassCondContoursHealthy.png}
		\caption{ML class conditional 2 density contours}
		\label{fig:cml2}
	\end{subfigure}
	\begin{subfigure}[b]{.40\textwidth}
		\includegraphics[width=\textwidth]{MLWONclassCondContoursDiseased.png}
		\caption{MLWON class conditional 1 density contours}
		\label{fig:cml3}
	\end{subfigure}
	\begin{subfigure}[b]{.40\textwidth}
		\includegraphics[width=\textwidth]{MLWONclassCondContoursHealthy.png}
		\caption{MLWON class conditional 2 density contours}
		\label{fig:cml4}
	\end{subfigure}
	\caption{Comparison of class conditional density contours for each class and with Naive and without Naive assumption}
	\label{fig:CMLdens}
\end{figure}

Moreover, the variance (covariance with 0 off diagonal elements) for diseased is relatively small when compared to healthy. Namely the chemical 2 variance for healthy. This means that chemical 2 (along the vertical axis) for healthy patients. We can see the affects of this in Figure \ref{fig:ML}, looking along the channel where chemical 1 = 4 to 5,  healthy patients are classified again above diseased. This is because healthy has a chemical 2 variance of 1.045 vs 0.176 for diseased. This difference is an order of magnitude and thus explains why we see healthy patients again.

\begin{table}[h!]
	\centering
	\caption{Class 1 (Diseased) variance of Chemical Concentrations for ML}
	\label{t:var1}
	\begin{tabular}{lrr}
		\hline
		\textbf{} & \textbf{Chemical 1} & \textbf{Chemical 2} \\ \hline
		Chemical 1 & 0.210 & 0.0 \\
		Chemical 2 & 0.0 & 0.176  \\
	\end{tabular}
\end{table}

\begin{table}[h!]
	\centering
	\caption{Class 2 (Healthy) variance of Chemical Concentrations for ML}
	\label{t:var2}
	\begin{tabular}{lrr}
		\hline
		\textbf{} & \textbf{Chemical 1} & \textbf{Chemical 2} \\ \hline
		Chemical 1 & 0.247 & 0.0 \\
		Chemical 2 & 0.0 & 1.045 \\
	\end{tabular}
\end{table}

Without the Naive assumption, we can see the dependence between the attributes on the healthy and diseased density contours in Figure \ref{fig:cml3} and \ref{fig:cml4}. However, even though we allow for dependence in this method, there is little relationship between the attributes for the healthy class. This is illustrated by the circular density contours.

The dependence of the attributes for density classification is shown by the tilted ellipse. This downward trend indicates a negative relationship between the attributes for diseased patients. The shape of the healthy and diseased contour plots is further explained by the covariance in Table \ref{t:cov1} and \ref{t:cov2}. Table \ref{t:cov1} is the covariance for diseased patients and confirms this negative relationship by negative off diagonal elements. Table \ref{t:cov2} also has negative off diagonal elements but are an order of magnitude smaller. This means the attributes have little affect on one another and confirms our assumption that they are essentially independent.

\begin{table}[h]
	\centering
	\caption{Class 1 Covariance of Chemical Concentrations for ML (or MAP) WON}
	\label{t:cov1}
	\begin{tabular}{lrr}
		\hline
		\textbf{} & \textbf{Chemical 1} & \textbf{Chemical 2} \\ \hline
		Chemical 1 & 0.210 & -0.124 \\
		Chemical 2 & -0.124 & 0.176  \\
	\end{tabular}
\end{table}

\begin{table}[h]
	\centering
	\caption{Class 2 Covariance of Chemical Concentrations for ML (or MAP) WON}
	\label{t:cov2}
	\begin{tabular}{lrr}
		\hline
		\textbf{} & \textbf{Chemical 1} & \textbf{Chemical 2} \\ \hline
		Chemical 1 & 0.247 & -0.023 \\
		Chemical 2 & -0.023 & 1.045  \\
	\end{tabular}
\end{table}

Considering the covariance for diseased as discussed previously, we can justify the trend of classification in Figure \ref{fig:MLWON}. The negative covariance can be seen by the tilted ellipse of diseased patients. Meaning that, if one of the chemicals has high concentration, the patient is likely diseased. If both are high, we start to see healthy patients again. The small values on the diagonal also explains the small spread of diseased patients, and why healthy patients start to dominate again. 

\section{Maximum Likelihood vs Maximum A Posteriori \\ (ML vs MAP)}{\label{s1}
In Task 2, we are asked to ``train the Bayesian classifier, with Maximum a Posteriori (ML), on the training data with and without the Naive assumption.'' The data set is the one shown in \ref{fig:trainingData}. Once trained we classify 2000 new data points (patients).

To train MAP with and without the Naive assumption is the same as ML, except we multiply the ML by the prior. Classifying with the Naive assumption is show in Figure \ref{fig:MAP}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{images/MAPnewData}
	\caption{Maximum A Posteriori classification of new data}
	\label{fig:MAP}
\end{figure}

Classifying without the Naive assumption is show in Figure \ref{fig:MAPWON}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{images/MAPWONnewData}
	\caption{Maximum A Posteriori without Naive assumption classification of new data}
	\label{fig:MAPWON}
\end{figure}
	
The main difference of ML and MAP is the prior in MAP. The prior allows us to specify belief about the data before we see it. For example, we can be biased against rate data, so we only classify it as rate data with a high likelihood. Conversely, we might want to increase the chance of detecting the rare class. We can set the prior to a high value for the rare class. This could be useful in this case as diseased classification is rare but we do not want to misclassifying as healthy when a patient is diseased. This means more healthy patients will be misclassified as diseased but as discussed before, we can test the patient again to confirm.

Several priors were trialled (Nc/N, 1/C, 0.99 vs 0.01), but varying the prior had little affect. This is due to the significant difference in means and small variances. In other words, the clusters of data points do not overlap so it is clear when a patient is diseased or not. 

If the clusters overlapped by some amount, ambiguity of classification is introduced. This means the prior will have more affect. The likelihood will be less 'certain' in classifying overlapped points, therefore, the prior would provide the bias needed to classify the preferred, or more likely, class.

Looking at Table \ref{t:classNumber}, we can see the difference in classification between the 4 methods. Looking at ML vs MAP, we can see the diseased has a difference of (826 - 764) 62. This is quite small and is expected for a data set with far apart means and small covariance. 



\begin{table}[h]
	\centering
	\caption{Classification Differences between Methods}
	\label{t:classNumber}
	\begin{tabular}{lrr}
		\hline
		\textbf{Method} & \textbf{Classified as Diseased} & \textbf{Classified as Healthy}\\ \hline
		ML & 764 & 1236 \\
		MLWON& 681 & 1319 \\
		MAP & 826 & 1174  \\
		MAPWON & 749 & 1254 \\
	\end{tabular}
\end{table}

Looking at Figure \ref{fig:overallC}, it is hard to notice any difference in classification. This is because of the small difference as discussed and show in Table \ref{t:classNumber}.

\begin{figure}[h!] 
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MLnewData.png}
		\caption{Maximum Likelihood classification of new data}
		\label{fig:model0}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MLWONnewData.png}
		\caption{Maximum Likelihood WON classification of new data}
		\label{fig:model1}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPnewData.png}
		\caption{Maximum A Posteriori classification of new data}
		\label{fig:model2}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPWONnewData.png}
		\caption{Maximum A Posteriori WON classification of new data}
		\label{fig:model3}
	\end{subfigure}
	\caption{Comparison of classification of new data by ML, MLWON, MAP, and MAPWON}
	\label{fig:overallC}
\end{figure}

\section{Conclusion}
The Naive assumption has a significant influence on this data set indicating there is a relationship between the attributes. However, this does not always mean it is the best method for classification. 

ML and MAP had little difference in this case due to limited affect of the prior. The affect was limited by the large difference in means and the small covariances.

Considering the hypothesis of the training data - an increase in concentration of chemical 1 and 2 increases the likelihood of a patient being diagnosed as diseased. 

With the Naive assumption, it almost agrees with our hypothesis. However, we notice that healthy patients appear again above the diseased, therefore, we reject the hypothesis.

Without the Naive assumption, there is a negative relationship between the attributes for diseased classification. In other words, if the concentration increases for one chemical but not the other, a patient is more likely diseased. Therefore, we reject the hypothesis for this case also.

\section{Further Comments}
It is safer to classify patients as diseased  and re test for confirmation than classify for healthy when a patient is diseased. In other words, a false negative is more costly than a false positive. This is reinforced by the speed and reduced cost of the urine test. Considering this, it may be better to use the naive assumption to classify. But when diagnosing it would be best to look at both and make an educated decision.

We also discover the rate at which the maximum likelihood decreases for diseased patients compared to healthy. Looking at Figures \ref{fig:cml1} and \ref{fig:cml3}, we see this by the close proximity of contours for the diseased compared to more spread of the healthy (or small covariance). This means, for an increase in chemical concentration, there is a point where the likelihood of being diseased becomes less than being healthy. This can be explained by imaging two lines with different slopes and y intercepts, where the y value is the likelihood of the class and slope represents the proximity of contour lines. If we imagine line 1 has a larger y intercept (larger initial likelihood) but greater negative slope (closer contour lines) than line 2, then there will be a point where y value of line 1 becomes less than line 2 due to the greater negative slope. 

The probability contours in Figures \ref{CMLprob} and \ref{CMAPprob} reinforce what we deduced in the density contour plots. This is because they are very much dependent on each other. To generate the probability contours we normalise the maximum likelihood by dividing it with the sum of the ML for each class. Therefore, the probability contour plots are the normalised versions of the maximum likelihood density plots. These are useful as the contours represent actual probabilities and also indicate the decision boundary for the 2 classes.

\begin{figure}[h!] 
	\centering
	\begin{subfigure}[b]{.40\textwidth}
		\includegraphics[width=\textwidth]{MLprobContoursDiseased.png}
		\caption{ML probability contours for class 1}
		\label{fig:model0}
	\end{subfigure}
	\begin{subfigure}[b]{0.40\textwidth}
		\includegraphics[width=\textwidth]{MLprobContoursHealthy.png}
		\caption{ML probability contours for class 2}
		\label{fig:model1}
	\end{subfigure}
	\begin{subfigure}[b]{.40\textwidth}
		\includegraphics[width=\textwidth]{MLWONprobContoursDiseased.png}
		\caption{MLWON probability contours for class 1}
		\label{fig:model2}
	\end{subfigure}
	\begin{subfigure}[b]{.40\textwidth}
		\includegraphics[width=\textwidth]{MLWONprobContoursHealthy.png}
		\caption{MLWON probability contours for class 2}
		\label{fig:model3}
	\end{subfigure}
	\caption{Comparison of probability contours for each class and with Naive and without Naive assumption}
	\label{fig:CMLprob}
\end{figure}

\begin{figure}[h!] 
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPprobContoursDiseased.png}
		\caption{MAP probability contours for class 1}
		\label{fig:model0}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPprobContoursHealthy.png}
		\caption{MAP probability contours for class 2}
		\label{fig:model1}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPWONprobContoursDiseased.png}
		\caption{MAPWON probability contours for class 1}
		\label{fig:model2}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPWONprobContoursHealthy.png}
		\caption{MAPWON probability contours for class 2}
		\label{fig:model3}
	\end{subfigure}
	\caption{Comparison of probability contours for each class and with Naive and without Naive assumption}
	\label{fig:CMAPprob}
\end{figure}

\begin{table}[h]
	\centering
	\caption{Mean of Chemical Concentrations for MAP (Identical to ML)}
	\label{t:ML}
	\begin{tabular}{lrr}
		\hline
		\textbf{Class} & \textbf{Chemical 1} & \textbf{Chemical 2} \\ \hline
		Diseased & 5.90 & 13.113 \\
		Healthy & 3.02 & 8.06  \\
	\end{tabular}
\end{table}

\begin{table}[h]
	\centering
	\caption{Class 1 Covariance of Chemical Concentrations for MAP WON}
	\label{t:ML}
	\begin{tabular}{lrr}
		\hline
		\textbf{} & \textbf{Chemical 1} & \textbf{Chemical 2} \\ \hline
		Chemical 1 & 0.210 & -0.124 \\
		Chemical 2 & -0.124 & 0.176  \\
	\end{tabular}
\end{table}

\begin{table}[h]
	\centering
	\caption{Class 2 Covariance of Chemical Concentrations for MAP WON}
	\label{t:ML}
	\begin{tabular}{lrr}
		\hline
		\textbf{} & \textbf{Chemical 1} & \textbf{Chemical 2} \\ \hline
		Chemical 1 & 0.247 & -0.023 \\
		Chemical 2 & -0.023 & 1.045  \\
	\end{tabular}
\end{table}

\begin{figure}[h!] 
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPtrainingData.png}
		\caption{Training data}
		\label{fig:modelNoReg0}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPnewData.png}
		\caption{Maximum A Posteriori classification of new data}
		\label{fig:modelNoReg1}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPWONnewData.png}
		\caption{Maximum A Posteriori without Naive assumption classification of new data}
		\label{fig:modelNoReg1}
	\end{subfigure}
	\caption{Comparison of training data and classification of new data by MAP and MAPWON}
	\label{fig:CMAP}
\end{figure}


\begin{figure}[h!] 
	\centering
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPclassCondContoursDiseased.png}
		\caption{MAP density contours for class 1}
		\label{fig:model0}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPclassCondContoursHealthy.png}
		\caption{MAP density contours for class 2}
		\label{fig:model1}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPWONclassCondContoursDiseased.png}
		\caption{MAPWON density contours for class 1}
		\label{fig:model2}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\includegraphics[width=\textwidth]{MAPWONclassCondContoursHealthy.png}
		\caption{MAPWON density contours for class 2}
		\label{fig:model3}
	\end{subfigure}
	\caption{Comparison of density contours for each class and with Naive and without Naive assumption}
	\label{fig:CMAPdens}
\end{figure}






